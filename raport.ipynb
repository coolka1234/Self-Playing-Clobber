{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f9ab9",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Raport z Projektu: Gra Clobber z AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8751abd",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Projekt implementuje grę logiczną Clobber wraz z agentami sztucznej inteligencji wykorzystującymi algorytmy Minimax oraz Alfa-Beta do podejmowania decyzji. Celem projektu było stworzenie funkcjonalnej gry, umożliwienie rozgrywki między agentami AI z różnymi heurystykami oraz analiza wydajności zastosowanych algorytmów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2c223",
   "metadata": {},
   "source": [
    "## Struktura Projektu\n",
    "\n",
    "Projekt składa się z kilku kluczowych modułów:\n",
    "\n",
    "*   `src/game_state.py`: Definiuje klasę `ClobberGameState` reprezentującą stan gry (plansza, aktualny gracz) oraz logikę ruchów i sprawdzania końca gry.\n",
    "*   `src/heuristics.py`: Zawiera funkcje heurystyczne oceniające stan gry (mobilność, liczba pionków, izolacja, ocena ogólna).\n",
    "*   `src/decision_tree.py`: Implementuje klasę `DecisionTree` odpowiedzialną za algorytmy Minimax i Alfa-Beta oraz mechanizm adaptacyjnej zmiany heurystyki.\n",
    "*   `src/game.py`: Definiuje klasę `ClobberAgent`, która integruje stan gry, heurystyki i drzewo decyzyjne do wyboru ruchu.\n",
    "*   `main.py`: Prosty skrypt do uruchomienia symulacji gry między dwoma agentami w jednym procesie.\n",
    "*   `playground.py`: Podobny do `main.py`, służy do eksperymentowania z różnymi konfiguracjami agentów.\n",
    "*   `player_W.py` / `player_B.py`: Implementują architekturę klient-serwer, gdzie każdy gracz działa jako osobny proces komunikujący się przez gniazda sieciowe.\n",
    "*   `raport.ipynb`: Niniejszy notatnik Jupyter służący jako raport."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4dfbf",
   "metadata": {},
   "source": [
    "## Kluczowe Komponenty Logiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf43094",
   "metadata": {},
   "source": [
    "### Stan Gry (`ClobberGameState`)\n",
    "\n",
    "Klasa `ClobberGameState` przechowuje aktualny układ pionków na planszy (reprezentowanej przez tablicę NumPy), informację o tym, który gracz ma ruch, oraz wymiary planszy. Udostępnia metody do:\n",
    "\n",
    "*   Tworzenia początkowej planszy (`create_clobber_board`).\n",
    "*   Generowania listy możliwych ruchów dla aktualnego gracza (`get_possible_moves`).\n",
    "*   Wykonywania ruchu na planszy (`make_move`).\n",
    "*   Sprawdzania, czy gra się zakończyła (`is_game_over`).\n",
    "*   Określania zwycięzcy (`check_winner`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ClobberGameState:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.board = self.create_clobber_board(rows, cols)\n",
    "        self.current_player = 'W'\n",
    "\n",
    "    def create_clobber_board(self, rows, cols):\n",
    "        board = np.empty((rows, cols), dtype=str)\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                board[r, c] = 'W' if (r + c) % 2 == 0 else 'B'\n",
    "        return board\n",
    "    \n",
    "    def get_possible_moves(self, print_moves=False):\n",
    "        \"\"\"\n",
    "        Get all possible moves for the current player.\n",
    "        \"\"\"\n",
    "        moves = []\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self.board[r, c] == self.current_player:\n",
    "                    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                        new_r, new_c = r + dr, c + dc\n",
    "                        if 0 <= new_r < self.rows and 0 <= new_c < self.cols:\n",
    "                            if self.board[new_r, new_c] != self.current_player and self.board[new_r, new_c] != '_':\n",
    "                                moves.append(((r, c), (new_r, new_c)))\n",
    "                                if print_moves:\n",
    "                                    print(\"Possible move:\", self.board[r, c], \"to\", self.board[new_r, new_c])\n",
    "        return moves\n",
    "    \n",
    "    def make_move(self, move):\n",
    "        \"\"\"\n",
    "        Make a move on the board.\n",
    "        \"\"\"\n",
    "        (start_r, start_c), (end_r, end_c) = move\n",
    "        if self.board[start_r, start_c] != self.current_player:\n",
    "            raise ValueError(\"Invalid move: not the current player's piece\")\n",
    "        if self.board[end_r, end_c] == self.current_player:\n",
    "            raise ValueError(\"Invalid move: cannot move to the same color\")\n",
    "        self.board[end_r, end_c] = self.current_player\n",
    "        self.board[start_r, start_c] = '_'\n",
    "        self.current_player = 'B' if self.current_player == 'W' else 'W'\n",
    "        return self\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        \"\"\"\n",
    "        Check if the game is over.\n",
    "        \"\"\"\n",
    "        return self.get_possible_moves() == []\n",
    "    \n",
    "    def check_winner(self):\n",
    "        if self.is_game_over():\n",
    "            if self.current_player == 'W':\n",
    "                return 'B'\n",
    "            else:\n",
    "                return 'W'\n",
    "        return None\n",
    "    \n",
    "    def get_num_of_pieces(self, player):\n",
    "        \"\"\"\n",
    "        Get the number of pieces for a player.\n",
    "        \"\"\"\n",
    "        return np.sum(self.board == player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d787b",
   "metadata": {},
   "source": [
    "### Heurystyki (`heuristics.py`)\n",
    "\n",
    "Moduł ten dostarcza funkcji oceniających jakość danego stanu gry z perspektywy konkretnego gracza. Zaimplementowane heurystyki to:\n",
    "\n",
    "*   `evaluate`: Kombinacja różnych czynników (mobilność, liczba pionków, izolacja) z wagami.\n",
    "*   `mobility_score`: Różnica w liczbie możliwych ruchów między graczem a przeciwnikiem.\n",
    "*   `piece_count_score`: Różnica w liczbie pionków na planszy.\n",
    "*   `isolation_score`: Różnica w liczbie izolowanych pionków przeciwnika i gracza (izolowany pionek nie ma sąsiadujących pionków żadnego koloru)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from game_state import ClobberGameState\n",
    "\n",
    "\n",
    "def evaluate(game_state : ClobberGameState, player):\n",
    "    opponent = 'B' if player == 'W' else 'W'\n",
    "    \n",
    "    def is_isolated(x, y):\n",
    "        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < len(game_state.board) and 0 <= ny < len(game_state.board[0]):\n",
    "                if game_state.board[nx][ny] in ('B', 'W'):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    my_pieces = opp_pieces = my_moves = opp_moves = my_isolated = opp_isolated = 0\n",
    "    for x in range(len(game_state.board)):\n",
    "        for y in range(len(game_state.board[0])):\n",
    "            piece = game_state.board[x][y]\n",
    "            if piece == player:\n",
    "                my_pieces += 1\n",
    "                if is_isolated(x, y): my_isolated += 1\n",
    "                for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < len(game_state.board) and 0 <= ny < len(game_state.board[0]):\n",
    "                        if game_state.board[nx][ny] == opponent:\n",
    "                            my_moves += 1\n",
    "            elif piece == opponent:\n",
    "                opp_pieces += 1\n",
    "                if is_isolated(x, y): opp_isolated += 1\n",
    "                for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                    nx, ny = x + dx, y + dy\n",
    "                    if 0 <= nx < len(game_state.board) and 0 <= ny < len(game_state.board[0]):\n",
    "                        if game_state.board[nx][ny] == player:\n",
    "                            opp_moves += 1\n",
    "\n",
    "    score = (\n",
    "        10 * (my_moves - opp_moves) +\n",
    "        20 * (my_pieces - opp_pieces) +\n",
    "        15 * (opp_isolated - my_isolated)\n",
    "    )\n",
    "    return score\n",
    "\n",
    "def mobility_score(game_state: ClobberGameState, player):\n",
    "    opponent = 'B' if player == 'W' else 'W'\n",
    "    def count_moves(p):\n",
    "        moves = 0\n",
    "        for x in range(len(game_state.board)):\n",
    "            for y in range(len(game_state.board[0])):\n",
    "                if game_state.board[x][y] == p:\n",
    "                    for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "                        nx, ny = x + dx, y + dy\n",
    "                        if 0 <= nx < len(game_state.board) and 0 <= ny < len(game_state.board[0]):\n",
    "                            if game_state.board[nx][ny] == ('B' if p == 'W' else 'W'):\n",
    "                                moves += 1\n",
    "        return moves\n",
    "    return count_moves(player) - count_moves(opponent)\n",
    "\n",
    "def piece_count_score(game_state: ClobberGameState, player):\n",
    "    opponent = 'B' if player == 'W' else 'W'\n",
    "    my_count= 0\n",
    "    opp_count= 0\n",
    "    for x in range(len(game_state.board)):\n",
    "        for y in range(len(game_state.board[0])):\n",
    "            piece = game_state.board[x][y]\n",
    "            if piece == player:\n",
    "                my_count += 1\n",
    "            elif piece == opponent:\n",
    "                opp_count += 1\n",
    "    return my_count - opp_count\n",
    "\n",
    "def isolation_score(game_state: ClobberGameState, player):\n",
    "    opponent = 'B' if player == 'W' else 'W'\n",
    "    \n",
    "    def is_isolated(x, y):\n",
    "        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < len(game_state.board) and 0 <= ny < len(game_state.board[0]):\n",
    "                if game_state.board[nx][ny] in ('B', 'W'):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def count_isolated(p):\n",
    "        count = 0\n",
    "        for x in range(len(game_state.board)):\n",
    "            for y in range(len(game_state.board[0])):\n",
    "                if game_state.board[x][y] == p and is_isolated(x, y):\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "    return count_isolated(opponent) - count_isolated(player)\n",
    "\n",
    "\n",
    "def heuristic_evaluate(game_state: ClobberGameState):\n",
    "    \"\"\"\n",
    "    Evaluate the game state for the given player.\n",
    "    \"\"\"\n",
    "    board = game_state.board\n",
    "    player = game_state.current_player\n",
    "    return evaluate(board, player)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d78bb4",
   "metadata": {},
   "source": [
    "### Drzewo Decyzyjne i Algorytmy (`DecisionTree`)\n",
    "\n",
    "Klasa `DecisionTree` jest sercem procesu decyzyjnego agentów AI. Implementuje:\n",
    "\n",
    "*   **Algorytm Minimax**: Przeszukuje drzewo gry, minimalizując maksymalną możliwą stratę.\n",
    "*   **Algorytm Alfa-Beta**: Optymalizacja algorytmu Minimax poprzez odcinanie gałęzi drzewa, które na pewno nie wpłyną na ostateczny wynik.\n",
    "*   **Ograniczenie Głębokości**: Przeszukiwanie jest ograniczone do zadanej głębokości (`max_depth`), aby zapewnić rozsądny czas obliczeń.\n",
    "*   **Buforowanie Heurystyki (Cache)**: Przechowuje wyniki obliczeń heurystyki dla już odwiedzonych stanów, aby uniknąć powtórnych obliczeń.\n",
    "*   **Adaptacyjna Zmiana Heurystyki**: (Opcjonalnie) Analizuje stan gry (np. etap gry - początkowy, środkowy, końcowy) i dynamicznie wybiera najbardziej odpowiednią heurystykę w danym momencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "from game_state import ClobberGameState\n",
    "from heuristics import mobility_score, piece_count_score, isolation_score\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth, game_state: ClobberGameState, heuristic, strategy='minmax', player=None):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree with the game state and strategy.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = self.crate_tree(game_state)\n",
    "        self.strategy = strategy\n",
    "        self.heuristic = heuristic\n",
    "        self.player = player\n",
    "        self.num_of_visits = 0\n",
    "        self.heuristic_cache = {}  \n",
    "\n",
    "    def crate_tree(self, game_state):\n",
    "        \"\"\"\n",
    "        Create a decision tree from the given game state.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(game_state, depth=0)\n",
    "        return self.tree\n",
    "\n",
    "    def _build_tree(self, game_state: ClobberGameState, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \"\"\"\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return None\n",
    "\n",
    "        if game_state.is_game_over():\n",
    "            return game_state.check_winner()\n",
    "\n",
    "        possible_moves = game_state.get_possible_moves(print_moves=False)\n",
    "\n",
    "        tree = {}\n",
    "        for move in possible_moves:\n",
    "            copied_game_state = copy.deepcopy(game_state)\n",
    "            new_game_state = copied_game_state.make_move(move)\n",
    "            tree[move] = self._build_tree(copy.deepcopy(new_game_state), depth + 1)\n",
    "\n",
    "        return tree\n",
    "    \n",
    "    def get_best_move(self, game_state: ClobberGameState):\n",
    "        \"\"\"\n",
    "        Get the best move for the current player using the heuristic.\n",
    "        \"\"\"\n",
    "        best_move_so_far = None\n",
    "        if self.strategy == 'minmax':\n",
    "            best_move_so_far = (None, float('-inf'))\n",
    "            for move in game_state.get_possible_moves():\n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                move_value = self.minimax_search(temp_game_state, self.max_depth - 1, False)\n",
    "                \n",
    "                if move_value > best_move_so_far[1]:\n",
    "                    best_move_so_far = (move, move_value)\n",
    "        elif self.strategy == 'alpha-beta':\n",
    "            best_move_so_far = (None, float('-inf'))\n",
    "            for move in game_state.get_possible_moves():\n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                move_value = self.alfa_beta_search(temp_game_state, self.max_depth - 1, float('-inf'), float('inf'), False)\n",
    "                \n",
    "                if move_value > best_move_so_far[1]:\n",
    "                    best_move_so_far = (move, move_value)\n",
    "        return best_move_so_far[0] if best_move_so_far else None\n",
    "    \n",
    "    def minimax_search(self, game_state: ClobberGameState, depth, maximizing_player):\n",
    "        \"\"\"\n",
    "        Perform a minimax search on the game state.\n",
    "        game_state: ClobberGameState\n",
    "        depth: Depth of the search\n",
    "        maximizing_player: Boolean indicating if it's the maximizing player's turn        \n",
    "        \"\"\"\n",
    "        state_key = (str(game_state.board), depth, maximizing_player)\n",
    "        if state_key in self.heuristic_cache:\n",
    "            return self.heuristic_cache[state_key]\n",
    "            \n",
    "        if depth == 0 or game_state.is_game_over():\n",
    "            result = self.heuristic(game_state, self.player)\n",
    "            self.heuristic_cache[state_key] = result\n",
    "            return result\n",
    "\n",
    "        possible_moves = game_state.get_possible_moves()\n",
    "        if not possible_moves:  \n",
    "            result = self.heuristic(game_state, self.player)\n",
    "            self.heuristic_cache[state_key] = result\n",
    "            return result\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for move in possible_moves:\n",
    "                self.num_of_visits += 1\n",
    "                \n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                eval_value = self.minimax_search(temp_game_state, depth - 1, False)\n",
    "                max_eval = max(max_eval, eval_value)\n",
    "            \n",
    "            self.heuristic_cache[state_key] = max_eval\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for move in possible_moves:\n",
    "                self.num_of_visits += 1\n",
    "                \n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                eval_value = self.minimax_search(temp_game_state, depth - 1, True)\n",
    "                min_eval = min(min_eval, eval_value)\n",
    "            \n",
    "            self.heuristic_cache[state_key] = min_eval\n",
    "            return min_eval\n",
    "    \n",
    "    def alfa_beta_search(self, game_state: ClobberGameState, depth, alpha, beta, maximizing_player):\n",
    "        \"\"\"\n",
    "        Perform an alpha-beta search on the game state.\n",
    "        \"\"\"\n",
    "        state_key = (str(game_state.board), depth, maximizing_player)\n",
    "        if state_key in self.heuristic_cache:\n",
    "            return self.heuristic_cache[state_key]\n",
    "            \n",
    "        if depth == 0 or game_state.is_game_over():\n",
    "            result = self.heuristic(game_state, self.player)\n",
    "            self.heuristic_cache[state_key] = result\n",
    "            return result\n",
    "\n",
    "        possible_moves = game_state.get_possible_moves(print_moves=False)\n",
    "        if not possible_moves:  \n",
    "            result = self.heuristic(game_state, self.player)\n",
    "            self.heuristic_cache[state_key] = result\n",
    "            return result\n",
    "\n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            for move in possible_moves:\n",
    "                self.num_of_visits += 1\n",
    "                \n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                eval_value = self.alfa_beta_search(temp_game_state, depth - 1, alpha, beta, False)\n",
    "                \n",
    "                max_eval = max(max_eval, eval_value)\n",
    "                alpha = max(alpha, eval_value)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            \n",
    "            self.heuristic_cache[state_key] = max_eval\n",
    "            return max_eval\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            for move in possible_moves:\n",
    "                self.num_of_visits += 1\n",
    "                \n",
    "                temp_game_state = copy.deepcopy(game_state)\n",
    "                temp_game_state.make_move(move)\n",
    "                \n",
    "                eval_value = self.alfa_beta_search(temp_game_state, depth - 1, alpha, beta, True)\n",
    "                \n",
    "                min_eval = min(min_eval, eval_value)\n",
    "                beta = min(beta, eval_value)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            \n",
    "            self.heuristic_cache[state_key] = min_eval\n",
    "            return min_eval\n",
    "    \n",
    "    def analyze_and_change_heuristic(self, game_state: ClobberGameState):\n",
    "        \"\"\"\n",
    "        Analyze the game state and change the heuristic based on the analysis.\n",
    "        Choose the most appropriate heuristic based on the current game state.\n",
    "        \"\"\"\n",
    "        mobility = mobility_score(game_state, self.player)\n",
    "        piece_count = piece_count_score(game_state, self.player)\n",
    "        isolation = isolation_score(game_state, self.player)\n",
    "        \n",
    "        total_pieces = game_state.get_num_of_pieces('W') + game_state.get_num_of_pieces('B')\n",
    "        max_pieces = game_state.cols * game_state.rows\n",
    "        game_progress = 1 - (total_pieces / max_pieces) \n",
    "        \n",
    "        epsilon = 1e-10\n",
    "        scores = {\n",
    "            \"mobility\": abs(mobility),\n",
    "            \"piece_count\": abs(piece_count),\n",
    "            \"isolation\": abs(isolation)\n",
    "        }\n",
    "        \n",
    "        max_score = max(scores.values()) + epsilon\n",
    "        normalized_scores = {k: v/max_score for k, v in scores.items()}\n",
    "        \n",
    "        if game_progress < 0.3:  # Erly game\n",
    "            weights = {\"mobility\": 0.7, \"piece_count\": 0.2, \"isolation\": 0.1}\n",
    "        elif game_progress < 0.7:  # midgame\n",
    "            weights = {\"mobility\": 0.4, \"piece_count\": 0.4, \"isolation\": 0.2}\n",
    "        else:  # Lategame\n",
    "            weights = {\"mobility\": 0.2, \"piece_count\": 0.3, \"isolation\": 0.5}\n",
    "        \n",
    "        weighted_scores = {\n",
    "            \"mobility\": normalized_scores[\"mobility\"] * weights[\"mobility\"],\n",
    "            \"piece_count\": normalized_scores[\"piece_count\"] * weights[\"piece_count\"],\n",
    "            \"isolation\": normalized_scores[\"isolation\"] * weights[\"isolation\"]\n",
    "        }\n",
    "        \n",
    "        best_heuristic_name = max(weighted_scores, key=weighted_scores.get)\n",
    "        \n",
    "        heuristic_map = {\n",
    "            \"mobility\": mobility_score,\n",
    "            \"piece_count\": piece_count_score,\n",
    "            \"isolation\": isolation_score\n",
    "        }\n",
    "        \n",
    "        new_heuristic = heuristic_map[best_heuristic_name]\n",
    "        \n",
    "        if self.heuristic.__code__ != new_heuristic.__code__:\n",
    "            old_heuristic_name = next((name for name, func in heuristic_map.items() \n",
    "                                      if func.__code__ == self.heuristic.__code__), \"unknown\")\n",
    "            \n",
    "            print(f\"Game progress: {game_progress:.2f} (Phase: {'early' if game_progress < 0.3 else 'mid' if game_progress < 0.7 else 'late'})\")\n",
    "            print(f\"Raw scores: Mobility={mobility:.2f}, Piece Count={piece_count:.2f}, Isolation={isolation:.2f}\")\n",
    "            print(f\"Normalized scores: {', '.join([f'{k}={v:.2f}' for k, v in normalized_scores.items()])}\")\n",
    "            print(f\"Weighted scores: {', '.join([f'{k}={v:.2f}' for k, v in weighted_scores.items()])}\")\n",
    "            print(f\"Changing heuristic from {old_heuristic_name} to {best_heuristic_name}\")\n",
    "            \n",
    "            return new_heuristic\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c79999",
   "metadata": {},
   "source": [
    "### Agent Gry (`ClobberAgent`)\n",
    "\n",
    "Klasa `ClobberAgent` łączy wszystkie komponenty. Inicjalizowana jest z określoną nazwą (np. 'W' lub 'B'), początkowym stanem gry, wybraną heurystyką, strategią (Minimax lub Alfa-Beta) i maksymalną głębokością przeszukiwania. Metoda `play` tej klasy:\n",
    "\n",
    "1.  Tworzy instancję `DecisionTree`.\n",
    "2.  (Opcjonalnie) Wywołuje mechanizm adaptacyjnej zmiany heurystyki.\n",
    "3.  Używa drzewa decyzyjnego do znalezienia najlepszego ruchu (`get_best_move`).\n",
    "4.  Wykonuje znaleziony ruch na obiekcie stanu gry.\n",
    "5.  Zwraca zaktualizowany stan gry lub `None`, jeśli gra się zakończyła."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "from game_state import ClobberGameState\n",
    "from decision_tree import DecisionTree\n",
    "class ClobberAgent:\n",
    "    def __init__(self, name, initial_game_state, heuristic, strategy='minmax',max_depth=None, adaptive=False):\n",
    "        self.name = name\n",
    "        self.game_state = initial_game_state\n",
    "        self.heuristic = heuristic\n",
    "        self.strategy = strategy\n",
    "        self.max_depth = max_depth\n",
    "        self.adaptive = adaptive\n",
    "\n",
    "    def play(self, game: ClobberGameState):\n",
    "        \"\"\"\n",
    "        Play a move using the decision tree strategy.\n",
    "        \"\"\"\n",
    "        print(f\"{self.name} is playing...\")\n",
    "        print(f\"Current board:\\n{game.board}\")\n",
    "        print(f\"Current player: {game.current_player}\")\n",
    "        print(f\"Evaluating moves using {self.strategy} strategy...\")\n",
    "\n",
    "        dt=DecisionTree(self.max_depth, copy.deepcopy(game), self.heuristic, self.strategy, self.name)\n",
    "\n",
    "        if self.adaptive:\n",
    "            potential_new_heuristic = dt.analyze_and_change_heuristic(copy.deepcopy(game))\n",
    "            self.heuristic= potential_new_heuristic if potential_new_heuristic else self.heuristic\n",
    "        best_move = dt.get_best_move(copy.deepcopy(game))\n",
    "        print(f\"Number of nodes visited: {dt.num_of_visits}\", file=sys.stderr)\n",
    "        if best_move:\n",
    "            game.make_move(best_move)\n",
    "            print(f\"{self.name} played move {best_move}\")\n",
    "        else:\n",
    "            print(f\"{self.name} has no valid moves. Game over.\")\n",
    "        winner = game.check_winner()\n",
    "        if winner:\n",
    "            print(f\"Winner: {winner}\")\n",
    "            print(f\"Final board:\\n{game.board}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"No winner yet.\")\n",
    "        return game\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba61ac",
   "metadata": {},
   "source": [
    "## Użyte Biblioteki\n",
    "\n",
    "*   **NumPy**: Do efektywnej reprezentacji i manipulacji planszą gry.\n",
    "*   **Copy**: Do tworzenia głębokich kopii obiektów stanu gry, co jest kluczowe w algorytmach przeszukiwania drzewa.\n",
    "*   **Multiprocessing**: (W `player_W.py` i `player_B.py`) Do komunikacji między procesami agentów za pomocą `Listener` i `Client`.\n",
    "*   **Sys**: Do zarządzania ścieżkami modułów i przekierowywania danych wyjściowych (np. liczby odwiedzonych węzłów na stderr).\n",
    "*   **Datetime**: Do mierzenia czasu wykonania algorytmów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3f675",
   "metadata": {},
   "source": [
    "## Uruchamianie Gry\n",
    "\n",
    "Grę można uruchomić na kilka sposobów:\n",
    "\n",
    "1.  **Symulacja w jednym procesie**: Uruchomienie `main.py` lub `playground.py`. Obaj agenci działają sekwencyjnie w tym samym procesie.\n",
    "2.  **Rozgrywka klient-serwer**: Należy uruchomić `player_B.py` (serwer/nasłuchujący), a następnie `player_W.py` (klient/łączący się). Każdy agent działa w osobnym procesie, komunikując się przez gniazda lokalne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c934079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "from src.game import ClobberAgent\n",
    "from src.game_state import ClobberGameState\n",
    "from src.heuristics import evaluate, mobility_score, piece_count_score, isolation_score\n",
    "\n",
    "def simulate_game(heuristic_W, heuristic_B, strategy_A, strategy_B, max_depth_A, max_depth_B, rows, cols):\n",
    "    \"\"\"\n",
    "    Simulate a game of Clobber between two agents.\n",
    "    \"\"\"\n",
    "    initial_game_state = ClobberGameState(rows,cols)\n",
    "    agent = ClobberAgent(name=\"W\", initial_game_state=initial_game_state, heuristic=heuristic_W, strategy=strategy_A, max_depth=max_depth_A)\n",
    "    agent2 = ClobberAgent(name=\"B\", initial_game_state=initial_game_state, heuristic=heuristic_B, strategy=strategy_B, max_depth=max_depth_B)\n",
    "    num_of_moves = 1\n",
    "    print(\"Starting game...\")\n",
    "    move=agent.play(initial_game_state)\n",
    "    while move:\n",
    "        num_of_moves += 1\n",
    "        move=agent2.play(move)\n",
    "        if move:\n",
    "            move=agent.play(move)\n",
    "        else:\n",
    "            break\n",
    "    print(\"Game Over\")\n",
    "    print(f\"Total moves played: {num_of_moves}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rows = 8\n",
    "    cols = 8\n",
    "    heuristic_W = isolation_score\n",
    "    heuristic_B = mobility_score\n",
    "    # alpha-beta/minmax\n",
    "    strategy_A = 'alpha-beta'\n",
    "    strategy_B = 'alpha-beta'\n",
    "    max_depth_A = 10\n",
    "    max_depth_B = 10\n",
    "\n",
    "    simulate_game(heuristic_W, heuristic_B, strategy_A, strategy_B, max_depth_A, max_depth_B, rows, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032e46b",
   "metadata": {},
   "source": [
    "## Wnioski i Możliwe Rozszerzenia\n",
    "\n",
    "Projekt skutecznie implementuje grę Clobber z agentami AI wykorzystującymi Minimax i Alfa-Beta. Kluczowe cechy to modularna struktura, różne heurystyki i możliwość adaptacyjnej zmiany strategii.\n",
    "\n",
    "Możliwe rozszerzenia:\n",
    "*   Implementacja bardziej zaawansowanych heurystyk.\n",
    "*   Dodanie graficznego interfejsu użytkownika (GUI).\n",
    "*   Optymalizacja algorytmów (np. Iterative Deepening Depth First Search - IDDFS)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
